We provide a reference implementation for the metrics and the ranking scheme so that participants know exactly how 
their algorithm is going to be validated. We strongly encourage using these implementations for model selection 
during development. In order to do this, we recommend training all your models as cross-validations, so that you have
predictions for the entire training set with which you can meaningfully evaluate your approaches.

# Compute metrics for your predictions
After installing the KiTS23 repository, the following console command is available to you: `kits23_compute_metrics`. 
You can use it to evaluate predictions against the training ground truth. It's as simple as 

`kits23_compute_metrics FOLDER_WITH_PREDICTIONS -num_processes XX`

This will produce a `evaluation.csv` in FOLDER_WITH_PREDICTIONS with the computed Dice and surface Dice scores. 

# Ranking code
Ranking is based on first averaging your dice and surface dice scores across all cases and HECs, resulting in two 
values: your average Dice and average Surface Dice. We then use 'rank-then-aggregate' to merge these metrics into a final
ranking.

Here are the steps you need to do to run the ranking locally (for example, to find the best configuration for 
your submission):

1) Execute `generate_summary_csv`, located in [ranking.py](ranking.py). The documentation will tell you how
2) Then use `rank_participants` from the same file with the summary.csv generated by `generate_summary_csv` to generate 
the final ranking.

Simple :-)
